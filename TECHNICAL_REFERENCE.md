# ğŸ”§ ReferÃªncia TÃ©cnica - LLM Pessoal

## ğŸ“‹ VisÃ£o Geral TÃ©cnica

A **LLM Pessoal** Ã© uma aplicaÃ§Ã£o web moderna construÃ­da com FastAPI que integra mÃºltiplos serviÃ§os de IA local. Esta documentaÃ§Ã£o tÃ©cnica fornece detalhes sobre arquitetura, APIs, configuraÃ§Ãµes e desenvolvimento.

## ğŸ—ï¸ Arquitetura do Sistema

### Diagrama de Arquitetura
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend       â”‚    â”‚   AI Services   â”‚
â”‚   (HTML/CSS/JS) â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚   (Ollama/SD)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Static Files  â”‚    â”‚   Templates     â”‚    â”‚   Cache/Data    â”‚
â”‚   (CSS/JS/IMG)  â”‚    â”‚   (Jinja2)      â”‚    â”‚   (Volumes)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principais

#### 1. Frontend (Interface Web)
- **Tecnologia**: HTML5, CSS3, JavaScript ES6+
- **Framework**: Vanilla JS com Fetch API
- **UI/UX**: Design responsivo com CSS Grid/Flexbox
- **Interatividade**: Streaming de respostas, tabs dinÃ¢micas

#### 2. Backend (FastAPI)
- **Framework**: FastAPI 0.115.14
- **Servidor**: Uvicorn ASGI
- **Templates**: Jinja2
- **ValidaÃ§Ã£o**: Pydantic
- **CORS**: Middleware configurado

#### 3. ServiÃ§os de IA
- **Ollama**: Servidor local para modelos LLM
- **Stable Diffusion**: Pipeline de geraÃ§Ã£o de imagens
- **PyTorch**: Framework de deep learning

## ğŸ”Œ APIs e Endpoints

### Endpoints Principais

#### Chat API
```python
POST /api/chat
{
    "message": "string",
    "model": "string",
    "stream": boolean
}
```

**Resposta (Streaming)**:
```json
{
    "response": "string",
    "done": boolean
}
```

#### GeraÃ§Ã£o de Imagens
```python
POST /api/generate-image
{
    "prompt": "string",
    "negative_prompt": "string",
    "width": integer,
    "height": integer,
    "num_inference_steps": integer,
    "guidance_scale": float
}
```

**Resposta**:
```json
{
    "image_url": "string",
    "metadata": {
        "prompt": "string",
        "parameters": object
    }
}
```

#### Status e Monitoramento
```python
GET /api/status
GET /api/health
GET /api/models
GET /api/models/detailed
```

### Exemplos de Uso da API

#### Chat com Streaming
```bash
curl -X POST "http://localhost:8001/api/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Explique a fotossÃ­ntese",
    "model": "llama3.2:latest",
    "stream": true
  }'
```

#### GeraÃ§Ã£o de Imagem
```bash
curl -X POST "http://localhost:8001/api/generate-image" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Um gato siamÃªs em um jardim japonÃªs",
    "negative_prompt": "borrÃ£o, baixa qualidade",
    "width": 512,
    "height": 512,
    "num_inference_steps": 20,
    "guidance_scale": 7.5
  }'
```

## âš™ï¸ ConfiguraÃ§Ãµes Detalhadas

### ConfiguraÃ§Ãµes do Servidor

```python
# app.py - ConfiguraÃ§Ãµes principais
HOST = "0.0.0.0"           # EndereÃ§o de escuta
PORT = 8001                 # Porta do servidor
DEBUG = False               # Modo debug
RELOAD = False              # Auto-reload (desenvolvimento)
WORKERS = 1                 # NÃºmero de workers
KEEP_ALIVE = 5             # Keep-alive timeout
```

### ConfiguraÃ§Ãµes Ollama

```python
# ConfiguraÃ§Ãµes do cliente Ollama
OLLAMA_HOST = "http://localhost:11434"
OLLAMA_TIMEOUT = 300       # Timeout em segundos
DEFAULT_MODEL = "llama3.2:latest"
MAX_TOKENS = 2048          # MÃ¡ximo de tokens por resposta
```

### ConfiguraÃ§Ãµes Stable Diffusion

```python
# ConfiguraÃ§Ãµes do pipeline SD
STABLE_DIFFUSION_MODEL = "runwayml/stable-diffusion-v1-5"
DEVICE = "auto"            # auto, cuda, cpu
ENABLE_MEMORY_EFFICIENT_ATTENTION = True
ENABLE_VAE_SLICING = True
```

### ConfiguraÃ§Ãµes de Performance

```python
# OtimizaÃ§Ãµes de memÃ³ria
PYTORCH_CUDA_ALLOC_CONF = "max_split_size_mb:128"
PYTORCH_ENABLE_MPS_FALLBACK = "1"
HF_HUB_DISABLE_SYMLINKS = "1"
HF_HUB_DISABLE_SYMLINKS_WARNING = "1"
```

## ğŸ—‚ï¸ Estrutura de Dados

### Modelos de Dados (Pydantic)

#### ChatRequest
```python
class ChatRequest(BaseModel):
    message: str
    model: str = "llama3.2:latest"
    stream: bool = True
```

#### ImageRequest
```python
class ImageRequest(BaseModel):
    prompt: str
    negative_prompt: str = ""
    width: int = 512
    height: int = 512
    num_inference_steps: int = 20
    guidance_scale: float = 7.5
```

### Estrutura de DiretÃ³rios

```
LLM-pessoal/
â”œâ”€â”€ app.py                 # Servidor principal
â”œâ”€â”€ config.py              # ConfiguraÃ§Ãµes centralizadas
â”œâ”€â”€ requirements.txt       # DependÃªncias Python
â”œâ”€â”€ docker-compose.yml     # ConfiguraÃ§Ã£o Docker
â”œâ”€â”€ Dockerfile            # Imagem Docker
â”œâ”€â”€ static/               # Arquivos estÃ¡ticos
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css     # Estilos CSS
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ app.js        # JavaScript
â”œâ”€â”€ templates/            # Templates HTML
â”‚   â””â”€â”€ index.html        # Template principal
â”œâ”€â”€ generated_images/     # Imagens geradas
â”œâ”€â”€ logs/                # Logs da aplicaÃ§Ã£o
â””â”€â”€ cache/               # Cache de modelos
    â”œâ”€â”€ huggingface/     # Cache HF
    â””â”€â”€ transformers/    # Cache Transformers
```

## ğŸ”§ Desenvolvimento

### Ambiente de Desenvolvimento

#### PrÃ©-requisitos
```bash
# Python 3.8+
python --version

# Pip atualizado
pip install --upgrade pip

# Git
git --version
```

#### ConfiguraÃ§Ã£o Local
```bash
# Clone o repositÃ³rio
git clone <url-do-repositorio>
cd LLM-pessoal

# Ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Instalar dependÃªncias
pip install -r requirements.txt

# Instalar Ollama
# Windows/Mac: https://ollama.com/
# Linux: curl -fsSL https://ollama.com/install.sh | sh
```

#### Executar em Desenvolvimento
```bash
# Modo debug
export DEBUG=true
export LOG_LEVEL=DEBUG
export RELOAD=true

# Executar
python app.py
```

### Debugging

#### Logs Detalhados
```python
# config.py
LOG_LEVEL = "DEBUG"
LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
```

#### Verificar ServiÃ§os
```bash
# Verificar Ollama
curl http://localhost:11434/api/version

# Verificar aplicaÃ§Ã£o
curl http://localhost:8001/api/health

# Ver logs
tail -f logs/app.log
```

### Testes

#### Testes Manuais
```bash
# Teste de chat
curl -X POST "http://localhost:8001/api/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello", "model": "phi3:mini"}'

# Teste de imagem
curl -X POST "http://localhost:8001/api/generate-image" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "test image"}'
```

#### Testes Automatizados (Futuro)
```python
# tests/test_api.py
import pytest
from fastapi.testclient import TestClient
from app import app

client = TestClient(app)

def test_chat_endpoint():
    response = client.post("/api/chat", json={
        "message": "Hello",
        "model": "phi3:mini"
    })
    assert response.status_code == 200
```

## ğŸ³ Docker

### Dockerfile
```dockerfile
FROM python:3.11-slim

# Instalar dependÃªncias do sistema
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Instalar PyTorch
RUN pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# Copiar cÃ³digo
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

# Expor porta
EXPOSE 8001

# Comando de inicializaÃ§Ã£o
CMD ["python", "app.py"]
```

### Docker Compose
```yaml
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  llm-app:
    build: .
    ports:
      - "8001:8001"
    volumes:
      - ./generated_images:/app/generated_images
      - ./logs:/app/logs
    depends_on:
      - ollama
```

## ğŸ”’ SeguranÃ§a

### ConfiguraÃ§Ãµes de SeguranÃ§a

#### CORS
```python
# ConfiguraÃ§Ã£o CORS
CORS_ORIGINS = ["*"]  # Em produÃ§Ã£o, especificar domÃ­nios
```

#### ValidaÃ§Ã£o de Entrada
```python
# Pydantic valida automaticamente
class ChatRequest(BaseModel):
    message: str
    model: str = "llama3.2:latest"
    stream: bool = True
```

#### Rate Limiting (Futuro)
```python
# ImplementaÃ§Ã£o futura
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
```

## ğŸ“Š Monitoramento

### MÃ©tricas DisponÃ­veis

#### Status dos ServiÃ§os
```python
GET /api/status
{
    "ollama_status": "online",
    "stable_diffusion_status": "online",
    "system_info": {
        "cpu_usage": 45.2,
        "memory_usage": 67.8,
        "gpu_usage": 23.1
    }
}
```

#### Logs Estruturados
```python
import logging
import json

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_entry = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName
        }
        return json.dumps(log_entry)
```

### Alertas e NotificaÃ§Ãµes

#### Health Checks
```python
@app.get("/api/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }
```

## ğŸš€ Performance

### OtimizaÃ§Ãµes Implementadas

#### MemÃ³ria
- **VAE Slicing**: Reduz uso de VRAM
- **Memory Efficient Attention**: OtimizaÃ§Ã£o de atenÃ§Ã£o
- **Gradient Checkpointing**: Economia de memÃ³ria

#### Velocidade
- **Streaming**: Respostas em tempo real
- **Caching**: Cache de modelos Hugging Face
- **Async/Await**: OperaÃ§Ãµes assÃ­ncronas

#### GPU
- **CUDA**: AceleraÃ§Ã£o GPU automÃ¡tica
- **Mixed Precision**: FP16 quando disponÃ­vel
- **Memory Pinning**: OtimizaÃ§Ã£o de transferÃªncia

### Benchmarks

#### Tempo de Resposta (Chat)
- **CPU (Intel i7)**: 2-5 segundos
- **GPU (RTX 3060)**: 0.5-2 segundos

#### Tempo de GeraÃ§Ã£o (Imagens)
- **CPU**: 2-5 minutos
- **GPU (6GB VRAM)**: 30-60 segundos
- **GPU (8GB+ VRAM)**: 15-30 segundos

## ğŸ”„ ManutenÃ§Ã£o

### Backup
```bash
# Backup de dados
tar -czf backup-$(date +%Y%m%d).tar.gz \
    generated_images/ \
    logs/ \
    cache/
```

### AtualizaÃ§Ãµes
```bash
# Atualizar cÃ³digo
git pull origin main

# Atualizar dependÃªncias
pip install -r requirements.txt --upgrade

# Reconstruir Docker
docker-compose build --no-cache
```

### Limpeza
```bash
# Limpar cache
rm -rf cache/huggingface/*
rm -rf cache/transformers/*

# Limpar logs antigos
find logs/ -name "*.log" -mtime +30 -delete

# Limpar imagens antigas
find generated_images/ -name "*.png" -mtime +7 -delete
```

---

*Esta documentaÃ§Ã£o tÃ©cnica Ã© atualizada regularmente conforme a aplicaÃ§Ã£o evolui.* 