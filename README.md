# LLM Pessoal ü§ñ

**Assistente Inteligente Local com IA Generativa**

Uma aplica√ß√£o web completa que integra **modelos de linguagem local** (via Ollama) e **gera√ß√£o de imagens** (Stable Diffusion) em uma interface moderna e intuitiva. Tudo funciona localmente, garantindo total privacidade dos seus dados.

## üåü Funcionalidades Principais

### üí¨ Chat Inteligente
- **Modelos Locais**: LLaMA 3.2, Phi-3 Mini, Mistral, Code Llama
- **Conversa√ß√£o Natural**: Interface de chat em tempo real
- **Streaming**: Respostas em tempo real com efeito de digita√ß√£o
- **Hist√≥rico**: Mant√©m conversas organizadas
- **M√∫ltiplos Modelos**: Troque entre modelos sem reiniciar

### üé® Gera√ß√£o de Imagens
- **Stable Diffusion**: Gera√ß√£o de imagens de alta qualidade
- **Controles Avan√ßados**: Ajuste de par√¢metros (passos, escala, tamanho)
- **Prompt Negativo**: Especifique o que n√£o deve aparecer
- **Download**: Salve imagens geradas localmente
- **M√∫ltiplos Formatos**: 512x512, 768x768, 1024x1024

### üñ•Ô∏è Interface Web Moderna
- **Design Responsivo**: Funciona em desktop e mobile
- **Tabs Organizadas**: Chat, Imagens e Configura√ß√µes
- **Status em Tempo Real**: Monitoramento de servi√ßos
- **Interface Intuitiva**: F√°cil de usar para iniciantes

### üîí Privacidade Total
- **100% Local**: Nenhum dado enviado para a internet
- **Sem Telemetria**: N√£o coleta informa√ß√µes de uso
- **Dados Seguros**: Tudo fica no seu computador

## üèóÔ∏è Arquitetura e Tecnologias

### Backend
- **FastAPI**: Framework web moderno e r√°pido
- **Uvicorn**: Servidor ASGI de alta performance
- **Pydantic**: Valida√ß√£o de dados e serializa√ß√£o
- **Jinja2**: Templates HTML din√¢micos

### IA e Machine Learning
- **Ollama**: Servidor local para modelos LLM
- **PyTorch**: Framework de deep learning
- **Diffusers**: Pipeline Stable Diffusion
- **Transformers**: Biblioteca Hugging Face
- **Accelerate**: Otimiza√ß√µes de performance

### Frontend
- **HTML5/CSS3**: Interface moderna e responsiva
- **JavaScript ES6+**: Interatividade e streaming
- **Font Awesome**: √çcones profissionais
- **CSS Grid/Flexbox**: Layout adaptativo

### Infraestrutura
- **Docker**: Containeriza√ß√£o completa
- **Docker Compose**: Orquestra√ß√£o de servi√ßos
- **Nginx**: Proxy reverso (opcional)
- **Volumes Persistentes**: Cache e dados

## üìã Pr√©-requisitos do Sistema

### M√≠nimos
- **Sistema Operativo**: Windows 10+, macOS 10.15+, Ubuntu 18.04+
- **RAM**: 8GB (16GB recomendado)
- **Armazenamento**: 10GB livres
- **Python**: 3.8+ (3.10+ recomendado)

### Recomendados
- **GPU NVIDIA**: RTX 2060+ com 6GB+ VRAM
- **RAM**: 16GB+ para modelos maiores
- **SSD**: Para melhor performance de cache
- **CPU**: 4+ cores para processamento

### Software Obrigat√≥rio
- **Ollama**: [ollama.com](https://ollama.com)
- **Docker**: [docker.com](https://docker.com) (opcional)
- **Git**: Para clonar o reposit√≥rio

## üöÄ Instala√ß√£o e Configura√ß√£o

### M√©todo 1: Docker (Recomendado)

```bash
# 1. Clone o reposit√≥rio
git clone <url-do-repositorio>
cd LLM-pessoal

# 2. Inicie com Docker Compose
docker-compose up -d

# 3. Acesse a aplica√ß√£o
# Abra: http://localhost:8001
```

**Vantagens do Docker:**
- ‚úÖ Instala√ß√£o autom√°tica de depend√™ncias
- ‚úÖ Configura√ß√£o isolada
- ‚úÖ Funciona em qualquer sistema
- ‚úÖ Baixa modelos automaticamente

### M√©todo 2: Instala√ß√£o Local

```bash
# 1. Clone o reposit√≥rio
git clone <url-do-repositorio>
cd LLM-pessoal

# 2. Crie ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# 3. Instale depend√™ncias
pip install -r requirements.txt

# 4. Instale Ollama
# Windows/Mac: Baixe de https://ollama.com/
# Linux:
curl -fsSL https://ollama.com/install.sh | sh

# 5. Baixe um modelo
ollama pull llama3.2:latest

# 6. Execute a aplica√ß√£o
python app.py
```

### M√©todo 3: Script de Inicializa√ß√£o

```bash
# Execute o script que faz tudo automaticamente
python run.py
```

## üì± Guia de Utiliza√ß√£o Passo a Passo

### 1. Primeira Execu√ß√£o

1. **Inicie a aplica√ß√£o** (Docker ou local)
2. **Aguarde a inicializa√ß√£o** (pode demorar 1-2 minutos)
3. **Verifique o status** na aba Configura√ß√µes
4. **Baixe um modelo** se necess√°rio

### 2. Chat com IA

#### Interface Principal
- **Seletor de Modelo**: Escolha entre modelos dispon√≠veis
- **√Årea de Chat**: Visualize conversas em tempo real
- **Campo de Mensagem**: Digite suas perguntas
- **Bot√£o Enviar**: Ou pressione Enter

#### Fluxo de Conversa
1. **Selecione um modelo** no dropdown
2. **Digite sua mensagem** no campo de texto
3. **Pressione Enter** ou clique em "Enviar"
4. **Aguarde a resposta** (aparece em streaming)
5. **Continue a conversa** naturalmente

#### Dicas de Uso
- **Prompts claros**: Seja espec√≠fico nas suas perguntas
- **Contexto**: O modelo lembra da conversa atual
- **Limpar hist√≥rico**: Use o bot√£o de lixeira quando necess√°rio
- **Trocar modelo**: Mude entre modelos sem perder contexto

### 3. Gera√ß√£o de Imagens

#### Interface de Imagens
- **Prompt Principal**: Descreva a imagem desejada
- **Prompt Negativo**: Especifique o que n√£o deve aparecer
- **Controles de Tamanho**: 512x512, 768x768, 1024x1024
- **Par√¢metros Avan√ßados**: Passos e escala de orienta√ß√£o

#### Fluxo de Gera√ß√£o
1. **V√° para a aba "Gera√ß√£o de Imagens"**
2. **Descreva a imagem** no campo principal
3. **Adicione prompt negativo** (opcional)
4. **Ajuste par√¢metros** se necess√°rio:
   - **Passos**: 10-50 (mais = melhor qualidade, mais lento)
   - **Escala**: 1-20 (mais = mais fiel ao prompt)
5. **Clique em "Gerar Imagem"**
6. **Aguarde a gera√ß√£o** (30-120 segundos)
7. **Fa√ßa download** da imagem gerada

#### Dicas para Imagens
- **Prompts detalhados**: "Um gato siam√™s sentado em um jardim japon√™s ao p√¥r do sol"
- **Prompts negativos**: "borr√£o, baixa qualidade, distor√ß√£o"
- **Tamanhos**: 512x512 √© mais r√°pido, 1024x1024 √© mais detalhado
- **Passos**: 20-30 √© um bom equil√≠brio

### 4. Configura√ß√µes e Monitoramento

#### Aba Configura√ß√µes
- **Status dos Servi√ßos**: Ollama e Stable Diffusion
- **Informa√ß√µes do Sistema**: Dispositivo, mem√≥ria, hist√≥rico
- **A√ß√µes**: Atualizar status, limpar hist√≥rico

#### Monitoramento
- **Verde**: Servi√ßo funcionando
- **Vermelho**: Servi√ßo com problema
- **Amarelo**: Servi√ßo carregando

## üîß Configura√ß√£o Avan√ßada

### Modelos Suportados

| Modelo | Tamanho | RAM | Velocidade | Qualidade | Uso Recomendado |
|--------|---------|-----|------------|-----------|------------------|
| `phi3:mini` | ~2GB | 4GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Chat r√°pido |
| `llama3.2:latest` | ~4GB | 8GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Uso geral |
| `llama3.1:latest` | ~4GB | 8GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Uso geral |
| `mistral:latest` | ~4GB | 8GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Chat avan√ßado |
| `codellama:latest` | ~4GB | 8GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Programa√ß√£o |

### Vari√°veis de Ambiente

Crie um arquivo `.env` para personalizar:

```env
# Servidor
HOST=0.0.0.0
PORT=8001
DEBUG=false
RELOAD=false

# Ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_TIMEOUT=300
DEFAULT_MODEL=llama3.2:latest
MAX_TOKENS=2048

# Stable Diffusion
STABLE_DIFFUSION_MODEL=runwayml/stable-diffusion-v1-5
DEVICE=auto  # auto, cuda, cpu

# Performance
WORKERS=1
KEEP_ALIVE=5
LOG_LEVEL=INFO

# Windows/WSL
WSL_OPTIMIZATION=true
```

### Configura√ß√£o GPU

#### NVIDIA CUDA
```bash
# Verificar CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Instalar PyTorch com CUDA
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

#### Configura√ß√£o Autom√°tica
A aplica√ß√£o detecta automaticamente:
- **GPU NVIDIA**: Usa CUDA se dispon√≠vel
- **GPU AMD**: Usa ROCm (se instalado)
- **CPU**: Fallback autom√°tico

### Otimiza√ß√µes de Performance

#### Para GPU
```env
DEVICE=cuda
ENABLE_MEMORY_EFFICIENT_ATTENTION=true
ENABLE_VAE_SLICING=true
```

#### Para CPU
```env
DEVICE=cpu
ENABLE_MEMORY_EFFICIENT_ATTENTION=false
```

## üêõ Resolu√ß√£o de Problemas

### Problemas Comuns

#### Ollama n√£o conecta
```bash
# Verificar se est√° rodando
ollama list

# Reiniciar servi√ßo
ollama serve

# Verificar porta
netstat -an | grep 11434
```

#### Erro de mem√≥ria
- **Solu√ß√£o**: Use modelos menores (`phi3:mini`)
- **Preven√ß√£o**: Feche outras aplica√ß√µes
- **Configura√ß√£o**: Use CPU em vez de GPU

#### Stable Diffusion lento
- **Reduzir tamanho**: Use 512x512
- **Menos passos**: 10-15 em vez de 20-30
- **Usar CPU**: Se VRAM insuficiente

#### Porta em uso
```bash
# Mudar porta
export PORT=8080
python app.py
```

### Logs e Debug

#### Verificar Logs
```bash
# Logs da aplica√ß√£o
tail -f logs/app.log

# Logs do Docker
docker-compose logs -f llm-app
```

#### Modo Debug
```env
DEBUG=true
LOG_LEVEL=DEBUG
```

### Problemas Espec√≠ficos

#### Windows
- **WSL**: Ative otimiza√ß√µes no `.env`
- **Antiv√≠rus**: Adicione exce√ß√µes para Python
- **Firewall**: Permita Python e Ollama

#### Linux
- **Permiss√µes**: `chmod +x run.py`
- **Depend√™ncias**: `sudo apt install python3-venv`

#### macOS
- **Homebrew**: `brew install ollama`
- **Permiss√µes**: Permita Python no Security

## üìÅ Estrutura do Projeto

```
LLM-pessoal/
‚îú‚îÄ‚îÄ app.py                 # Servidor principal FastAPI
‚îú‚îÄ‚îÄ config.py              # Configura√ß√µes centralizadas
‚îú‚îÄ‚îÄ run.py                 # Script de inicializa√ß√£o
‚îú‚îÄ‚îÄ requirements.txt       # Depend√™ncias Python
‚îú‚îÄ‚îÄ docker-compose.yml     # Configura√ß√£o Docker
‚îú‚îÄ‚îÄ Dockerfile            # Imagem Docker
‚îú‚îÄ‚îÄ .env                  # Vari√°veis de ambiente (criar)
‚îú‚îÄ‚îÄ static/               # Arquivos est√°ticos
‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css     # Estilos da interface
‚îÇ   ‚îî‚îÄ‚îÄ js/
‚îÇ       ‚îî‚îÄ‚îÄ app.js        # JavaScript da aplica√ß√£o
‚îú‚îÄ‚îÄ templates/            # Templates HTML
‚îÇ   ‚îî‚îÄ‚îÄ index.html        # Interface principal
‚îú‚îÄ‚îÄ generated_images/     # Imagens geradas
‚îú‚îÄ‚îÄ logs/                # Logs da aplica√ß√£o
‚îú‚îÄ‚îÄ cache/               # Cache de modelos
‚îÇ   ‚îú‚îÄ‚îÄ huggingface/     # Cache HF
‚îÇ   ‚îî‚îÄ‚îÄ transformers/    # Cache Transformers
‚îî‚îÄ‚îÄ docker-data/         # Dados persistentes Docker
    ‚îú‚îÄ‚îÄ ollama/          # Modelos Ollama
    ‚îú‚îÄ‚îÄ huggingface/     # Cache HF
    ‚îî‚îÄ‚îÄ transformers/    # Cache Transformers
```

## üîÑ Atualiza√ß√µes e Manuten√ß√£o

### Atualizar Aplica√ß√£o
```bash
# Backup (opcional)
cp -r generated_images/ backup_images/

# Atualizar c√≥digo
git pull origin main

# Reinstalar depend√™ncias
pip install -r requirements.txt --upgrade

# Reiniciar
python app.py
```

### Atualizar Modelos
```bash
# Listar modelos
ollama list

# Atualizar modelo
ollama pull llama3.2:latest

# Remover modelo antigo
ollama rm llama3.1:latest
```

### Limpeza de Cache
```bash
# Limpar cache Python
pip cache purge

# Limpar cache HF
rm -rf cache/huggingface/*
rm -rf cache/transformers/*
```

## ü§ù Contribui√ß√µes

### Como Contribuir
1. **Fork** o reposit√≥rio
2. **Crie uma branch**: `git checkout -b feature/nova-funcionalidade`
3. **Commit**: `git commit -m 'Adiciona nova funcionalidade'`
4. **Push**: `git push origin feature/nova-funcionalidade`
5. **Pull Request**: Abra um PR com descri√ß√£o detalhada

### √Åreas para Contribui√ß√£o
- **Novos modelos**: Adicionar suporte a outros LLMs
- **Interface**: Melhorias na UI/UX
- **Performance**: Otimiza√ß√µes de velocidade
- **Documenta√ß√£o**: Melhorar guias e exemplos
- **Testes**: Adicionar testes automatizados

## üìú Licen√ßa

Este projeto est√° licenciado sob a **Licen√ßa MIT**. Veja o arquivo `LICENSE` para detalhes.

## üÜò Suporte e Comunidade

### Canais de Ajuda
- **Issues GitHub**: Para bugs e problemas
- **Discussions**: Para perguntas e discuss√µes
- **Wiki**: Documenta√ß√£o detalhada

### Informa√ß√µes de Debug
Ao reportar problemas, inclua:
- **Sistema Operativo**: Windows/Linux/macOS
- **Vers√£o Python**: `python --version`
- **Logs**: Conte√∫do de `logs/app.log`
- **Configura√ß√£o**: Conte√∫do do `.env`
- **Passos**: Como reproduzir o problema

## üîÆ Roadmap

### Pr√≥ximas Funcionalidades
- [ ] **Voz**: Reconhecimento e s√≠ntese de voz
- [ ] **M√∫ltiplos Usu√°rios**: Sistema de contas
- [ ] **Plugins**: Sistema de extens√µes
- [ ] **API REST**: Endpoints para integra√ß√£o
- [ ] **Mobile App**: Aplica√ß√£o nativa

### Melhorias Planejadas
- [ ] **Cache Inteligente**: Otimiza√ß√£o de mem√≥ria
- [ ] **Modelos Customizados**: Fine-tuning local
- [ ] **Interface Avan√ßada**: Mais controles de imagem
- [ ] **Backup Autom√°tico**: Sistema de backup

---

**Desenvolvido com ‚ù§Ô∏è para a comunidade portuguesa**

*Uma ferramenta poderosa para explorar IA local de forma segura e privada.* 