# ğŸš€ Enhanced LLM Platform - NetworkChuck Style AI Hub

Uma aplicaÃ§Ã£o web **muito potente** para IA local e em nuvem, inspirada no tutorial do [NetworkChuck](https://youtu.be/Wjrdr0NU4Sk). Esta Ã© uma plataforma completa de IA que conecta mÃºltiplos provedores atravÃ©s de uma interface unificada com controle total sobre usuÃ¡rios, orÃ§amentos e monitoramento.

---

## âœ¨ Funcionalidades Principais

### ğŸ¯ **Seguindo os Passos do NetworkChuck:**
- **Multiple AI Providers**: OpenAI, Anthropic (Claude), Google (Gemini), Groq, Ollama
- **LiteLLM Integration**: Proxy inteligente para conectar todos os provedores
- **Budget Control**: Sistema de orÃ§amento por usuÃ¡rio com alertas
- **User Management**: Roles, permissions, grupos e monitoramento
- **Real-time Monitoring**: Analytics completas de uso e custos
- **Multi-Model Comparison**: Compare respostas lado a lado
- **Local + Cloud**: Ollama local + APIs cloud em uma interface

### ğŸ”¥ **Funcionalidades AvanÃ§adas:**
- **Streaming Responses**: Respostas em tempo real
- **Chat Sessions**: HistÃ³rico persistente de conversas
- **Usage Analytics**: Dashboards com mÃ©tricas detalhadas
- **Role-based Access**: Admin, Premium, Standard, Basic
- **Guardrails**: Sistema de prompts e limitaÃ§Ãµes personalizÃ¡veis
- **Export/Import**: Backup de conversas em JSON
- **API Gateway**: LiteLLM como proxy para todas as APIs
- **Cost Tracking**: Monitoramento preciso de custos por token

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Flask App     â”‚â”€â”€â”€â”€â”‚    LiteLLM      â”‚â”€â”€â”€â”€â”‚  AI Providers   â”‚
â”‚   (Port 5000)   â”‚    â”‚   (Port 4000)   â”‚    â”‚  (OpenAI, etc)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     Redis       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚   (Port 6379)   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚    Ollama       â”‚â”€â”€â”€â”€â”€â”€â”
                        â”‚  (Port 11434)   â”‚      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                 â”‚               â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Open WebUI    â”‚  â”‚ Stable Diffusionâ”‚
                   â”‚   (Port 8080)   â”‚  â”‚   (Port 7860)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Whisper API    â”‚
                        â”‚   (Port 5001)   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ InstalaÃ§Ã£o RÃ¡pida (Docker)

### 1. Clone o repositÃ³rio
```bash
git clone <repository-url>
cd LLM-Sharkcoders
```

### 2. Configure as variÃ¡veis de ambiente
```bash
cp config.env.example config.env
# Edite config.env com suas API keys
```

### 3. Execute com Docker Compose
```bash
# Executar todos os serviÃ§os (requer GPU para melhor performance)
docker-compose up -d

# Ou executar apenas serviÃ§os especÃ­ficos
docker-compose up -d app litellm ollama redis          # ServiÃ§os bÃ¡sicos
docker-compose up -d open-webui                        # Interface Ollama
docker-compose up -d stable-diffusion-webui            # GeraÃ§Ã£o de imagens
docker-compose up -d whisper-api                       # TranscriÃ§Ã£o de Ã¡udio
```

**Nota**: Os serviÃ§os que requerem GPU (Ollama, Stable Diffusion, Whisper) estÃ£o configurados para deteÃ§Ã£o automÃ¡tica de GPU NVIDIA. Se nÃ£o tiver GPU, remova ou comente as secÃ§Ãµes `deploy.resources` no `docker-compose.yml`.

### 4. Acesse a aplicaÃ§Ã£o
- **App Principal**: http://localhost:5000
- **LiteLLM UI**: http://localhost:4000 (admin/admin123)
- **Ollama**: http://localhost:11434
- **Open WebUI**: http://localhost:8080 (Interface moderna para Ollama)
- **Stable Diffusion WebUI**: http://localhost:7860 (GeraÃ§Ã£o de imagens)
- **Whisper API**: http://localhost:5001 (TranscriÃ§Ã£o de Ã¡udio)

---

## ğŸ”§ ConfiguraÃ§Ã£o Manual

### 1. Instale as dependÃªncias
```bash
python -m pip install -r requirements.txt
```

### 2. Configure o ambiente
```bash
# Copie e edite o arquivo de configuraÃ§Ã£o
cp config.env.example .env
```

### 3. Configure os serviÃ§os externos

#### **Redis** (Para cache e rate limiting)
```bash
# Ubuntu/Debian
sudo apt install redis-server
redis-server

# Docker
docker run -d --name redis -p 6379:6379 redis:alpine
```

#### **Ollama** (Para modelos locais)
```bash
# Instale Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Baixe modelos
ollama pull llama3
ollama pull mistral
ollama pull codellama
```

#### **LiteLLM** (Proxy para APIs)
```bash
# Clone e configure LiteLLM
git clone https://github.com/BerriAI/litellm.git
cd litellm
pip install -e .

# Execute com nossa configuraÃ§Ã£o
litellm --config ../litellm_config.yaml --port 4000
```

### 4. Execute a aplicaÃ§Ã£o
```bash
python app.py
```

---

## ğŸ‘¥ Sistema de UsuÃ¡rios

### **Roles e PermissÃµes:**

| Role | Budget MÃ¡ximo | Modelos DisponÃ­veis | PermissÃµes Especiais |
|------|---------------|---------------------|---------------------|
| **Admin** | $1000/mÃªs | Todos | Gerenciar usuÃ¡rios, ver todos os chats |
| **Premium** | $100/mÃªs | OpenAI, Claude, Gemini, Ollama | Acesso completo aos modelos |
| **Standard** | $20/mÃªs | OpenAI, Ollama | Modelos bÃ¡sicos |
| **Basic** | $0/mÃªs | Apenas Ollama | Apenas modelos locais |

### **UsuÃ¡rio Admin PadrÃ£o:**
- **Username**: `admin`
- **Password**: `admin123` 
- **âš ï¸ MUDE A SENHA EM PRODUÃ‡ÃƒO!**

---

## ğŸ® Como Usar

### **1. Dashboard Principal**
- Visualize seu uso atual e orÃ§amento
- Acesse estatÃ­sticas de modelos utilizados
- Navegue entre funcionalidades

### **2. Chat Individual**
- Escolha qualquer modelo disponÃ­vel
- Streaming de respostas em tempo real
- HistÃ³rico de conversas salvo automaticamente

### **3. ComparaÃ§Ã£o Multi-Modelo**
- Teste a mesma pergunta em vÃ¡rios modelos
- Compare qualidade e velocidade
- Analise custos por resposta

### **4. Painel Admin** (Apenas admins)
- Gerencie usuÃ¡rios e permissÃµes
- Configure orÃ§amentos e limites
- Monitore uso do sistema
- Visualize analytics detalhadas

---

## ğŸ’° Controle de Custos

### **Como Funciona:**
1. **APIs por Token**: Pague apenas pelo que usar
2. **Budgets por UsuÃ¡rio**: Limite gastos mensais
3. **Alertas AutomÃ¡ticos**: Avisos quando prÃ³ximo ao limite
4. **Analytics Detalhadas**: Veja exatamente onde gasta

### **Custos Aproximados:**
- **GPT-4o**: ~$5-15/mÃªs para uso moderado
- **Claude 3.5**: ~$3-10/mÃªs para uso moderado  
- **Gemini**: ~$1-5/mÃªs para uso moderado
- **Ollama**: $0 (local, apenas custos de energia)

---

## ğŸ¨ **ServiÃ§os Adicionais de IA**

### **ğŸŒ Open WebUI (Porta 8080)**
Interface moderna e intuitiva para interagir com modelos Ollama:
- **Funcionalidades**: Chat avanÃ§ado, gestÃ£o de modelos, interface responsiva
- **Acesso**: http://localhost:8080
- **Conectividade**: IntegraÃ§Ã£o direta com Ollama

### **ğŸ­ Stable Diffusion WebUI (Porta 7860)**
GeraÃ§Ã£o de imagens com IA usando Stable Diffusion:
- **Funcionalidades**: Text-to-image, image-to-image, inpainting
- **Acesso**: http://localhost:7860
- **Requisitos**: GPU NVIDIA recomendada (configuraÃ§Ã£o automÃ¡tica)
- **Modelos**: Download automÃ¡tico na primeira execuÃ§Ã£o

### **ğŸ—£ï¸ Whisper API (Porta 5001)**
TranscriÃ§Ã£o de Ã¡udio para texto usando Whisper.cpp:
- **Endpoint**: `POST /transcribe`
- **Formato**: Envio de ficheiros de Ã¡udio via form-data
- **Modelo**: Base English (pode ser alterado)
- **Exemplo de uso**:
```bash
curl -X POST -F "audio=@audio_file.wav" http://localhost:5001/transcribe
```

## ğŸ”’ SeguranÃ§a e Privacidade

### **Funcionalidades de SeguranÃ§a:**
- **Guardrails**: Prompts de sistema para filtrar conteÃºdo
- **Rate Limiting**: PrevenÃ§Ã£o de spam e abuso
- **Role-based Access**: Controle granular de permissÃµes
- **Chat Monitoring**: Admins podem monitorar conversas (se habilitado)
- **Budget Limits**: PrevenÃ§Ã£o de gastos excessivos

### **Dados Locais:**
- Modelos Ollama rodam 100% localmente
- Stable Diffusion executa localmente
- Whisper API processa Ã¡udio localmente
- HistÃ³rico de chats armazenado localmente
- Controle total sobre seus dados

---

## ğŸ› ï¸ Comandos CLI

```bash
# Criar usuÃ¡rio admin
python app.py create-admin

# Resetar uso mensal de todos os usuÃ¡rios
python app.py reset-usage

# Listar todos os usuÃ¡rios
python app.py list-users

# Migrar banco de dados
flask db upgrade
```

---

## ğŸ“Š Monitoramento

### **MÃ©tricas DisponÃ­veis:**
- Requests por dia/usuÃ¡rio/modelo
- Custos detalhados por provider
- Tempo de resposta mÃ©dio
- Taxa de sucesso/erro
- Uso de tokens por modelo
- PadrÃµes de uso por hora

### **Dashboards:**
- **User Dashboard**: EstatÃ­sticas pessoais
- **Admin Dashboard**: VisÃ£o geral do sistema
- **Analytics**: MÃ©tricas detalhadas e grÃ¡ficos

---

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### **VariÃ¡veis de Ambiente Importantes:**
```bash
# AI Providers
OPENAI_API_KEY=your-key
ANTHROPIC_API_KEY=your-key
GOOGLE_API_KEY=your-key
GROQ_API_KEY=your-key

# LiteLLM
LITELLM_MASTER_KEY=your-master-key
LITELLM_SALT_KEY=your-salt-key

# Application
DEFAULT_USER_BUDGET=20.00
MAX_CHAT_HISTORY=100
ENABLE_USAGE_TRACKING=True
```

### **PersonalizaÃ§Ã£o de Modelos:**
Edite `app/config.py` para:
- Adicionar novos modelos
- Ajustar custos por token
- Modificar permissÃµes de roles
- Configurar limites de budget

---

## ğŸš¨ Troubleshooting

### **Problemas Comuns:**

**1. LiteLLM nÃ£o conecta:**
```bash
# Verifique se as API keys estÃ£o configuradas
docker-compose logs litellm
```

**2. Ollama models nÃ£o carregam:**
```bash
# Baixe os modelos manualmente
ollama pull llama3
ollama list
```

**3. Redis connection error:**
```bash
# Verifique se Redis estÃ¡ rodando
docker-compose logs redis
redis-cli ping
```

**4. Budget nÃ£o atualiza:**
```bash
# Reset manual se necessÃ¡rio
python app.py reset-usage
```

---

## ğŸ¯ Roadmap

### **PrÃ³ximas Funcionalidades:**
- [ ] **Voice Chat**: IntegraÃ§Ã£o com Whisper e TTS
- [ ] **Document Upload**: RAG com PDFs e documentos
- [ ] **Custom Models**: Fine-tuning de modelos locais
- [ ] **Teams**: Workspaces colaborativos
- [ ] **API Keys**: API externa para integraÃ§Ã£o
- [ ] **Plugins**: Sistema de extensÃµes
- [ ] **Mobile App**: React Native companion

---

## ğŸ† CrÃ©ditos

### **Inspirado por:**
- **[NetworkChuck](https://www.youtube.com/@NetworkChuck)** - Tutorial original e arquitetura
- **[LiteLLM](https://github.com/BerriAI/litellm)** - Proxy inteligente para APIs
- **[Ollama](https://ollama.ai/)** - ExecuÃ§Ã£o local de modelos
- **[Open WebUI](https://github.com/open-webui/open-webui)** - Interface inspiraÃ§Ã£o

### **Tecnologias:**
- **Flask**: Framework web Python
- **SQLAlchemy**: ORM para banco de dados
- **Redis**: Cache e rate limiting
- **Docker**: ContainerizaÃ§Ã£o
- **Chart.js**: GrÃ¡ficos e analytics
- **Font Awesome**: Ãcones

---

## ğŸ“ LicenÃ§a

MIT License - Veja [LICENSE](LICENSE) para detalhes.

---

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

---

## ğŸ’¬ Suporte

- **Issues**: Reporte bugs e solicite features
- **Discussions**: Tire dÃºvidas e compartilhe ideias
- **Discord**: [Link do servidor] (se houver)

---

**ğŸ‰ Agora vocÃª tem seu prÃ³prio AI Hub seguindo o estilo NetworkChuck! Divirta-se explorando o poder da IA! ğŸš€** 