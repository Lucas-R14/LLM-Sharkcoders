# env.example
# Configurações de ambiente para LLM Pessoal
# Copie para .env e ajuste conforme necessário

# ==============================================
# CONFIGURAÇÕES DO SERVIDOR
# ==============================================
HOST=0.0.0.0
PORT=8001
DEBUG=false
RELOAD=false
LOG_LEVEL=INFO

# ==============================================
# CONFIGURAÇÕES OLLAMA
# ==============================================
OLLAMA_HOST=http://ollama:11434
OLLAMA_TIMEOUT=300
DEFAULT_MODEL=llama3.2:latest
MAX_TOKENS=2048

# ==============================================
# CONFIGURAÇÕES STABLE DIFFUSION
# ==============================================
STABLE_DIFFUSION_MODEL=runwayml/stable-diffusion-v1-5
DEVICE=auto
# Opções: auto, cuda, cpu

# ==============================================
# CONFIGURAÇÕES DE CACHE E STORAGE
# ==============================================
HF_HUB_CACHE=/app/cache/huggingface
TRANSFORMERS_CACHE=/app/cache/transformers
HF_HUB_DISABLE_SYMLINKS=1
HF_HUB_DISABLE_SYMLINKS_WARNING=1

# ==============================================
# CONFIGURAÇÕES PYTORCH
# ==============================================
PYTORCH_ENABLE_MPS_FALLBACK=1
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# ==============================================
# CONFIGURAÇÕES DE SEGURANÇA
# ==============================================
MAX_CHAT_HISTORY=50
MAX_IMAGE_SIZE=1024
CORS_ORIGINS=*

# ==============================================
# CONFIGURAÇÕES DE PERFORMANCE
# ==============================================
WORKERS=1
KEEP_ALIVE=5
WSL_OPTIMIZATION=true

# ==============================================
# CONFIGURAÇÕES DE DESENVOLVIMENTO
# ==============================================
# Para desenvolvimento local
# OLLAMA_HOST=http://localhost:11434
# HOST=127.0.0.1
# DEBUG=true
# RELOAD=true
# LOG_LEVEL=DEBUG

# ==============================================
# CONFIGURAÇÕES AVANÇADAS
# ==============================================
# Auto download de modelos na inicialização
AUTO_DOWNLOAD_MODELS=true

# Modelos específicos para baixar
MODELS_TO_DOWNLOAD=llama3.2:latest,phi3:mini

# Configurações de rede Docker
DOCKER_SUBNET=172.20.0.0/16

# ==============================================
# CONFIGURAÇÕES GPU/CUDA (se disponível)
# ==============================================
# CUDA_VISIBLE_DEVICES=0
# NVIDIA_VISIBLE_DEVICES=all
# NVIDIA_DRIVER_CAPABILITIES=compute,utility 