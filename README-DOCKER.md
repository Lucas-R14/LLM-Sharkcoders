# üê≥ LLM Pessoal - Guia Docker Completo

## üìã Vis√£o Geral

Esta vers√£o da **LLM Pessoal** funciona 100% em containers Docker, oferecendo uma solu√ß√£o completa e isolada que resolve problemas de depend√™ncias e configura√ß√£o. Inclui:

- **ü§ñ Ollama**: Modelos LLM locais (llama3.2, phi3, mistral, etc.)
- **üé® Stable Diffusion**: Gera√ß√£o de imagens de alta qualidade
- **üåê Interface Web**: Chat moderno e controle completo
- **‚öôÔ∏è Configura√ß√£o Autom√°tica**: Tudo pronto para usar
- **üìä Monitoramento**: Status em tempo real dos servi√ßos

## üöÄ In√≠cio R√°pido

### Windows

1. **Instalar Docker Desktop**
   ```
   https://www.docker.com/products/docker-desktop/
   ```

2. **Executar aplica√ß√£o**
   ```batch
   # M√©todo 1: Script Batch (mais simples)
   start-docker.bat
   
   # M√©todo 2: PowerShell (mais controle)
   .\start-docker.ps1
   
   # M√©todo 3: Manual
   docker-compose up -d
   ```

3. **Acessar interface**
   - üåê **Interface principal**: http://localhost:8001
   - üìä **Status**: http://localhost:8001/api/status
   - üîß **Ollama**: http://localhost:11434

### Linux/macOS

```bash
# Clonar e iniciar
git clone <seu-repo>
cd "LLM pessoal"

# Tornar script execut√°vel
chmod +x docker-entrypoint.sh

# Iniciar aplica√ß√£o
docker-compose up -d

# Ver logs
docker-compose logs -f
```

## üèóÔ∏è Arquitetura Docker

### Servi√ßos Principais

#### 1. Ollama (ollama)
- **Imagem**: `ollama/ollama:latest`
- **Porta**: 11434
- **Fun√ß√£o**: Executar modelos LLM locais
- **Volume**: `docker-data/ollama`
- **Healthcheck**: Verifica se est√° funcionando

#### 2. LLM App (llm-app)
- **Build**: Dockerfile local
- **Porta**: 8001
- **Fun√ß√£o**: Interface web e API
- **Volumes**: c√≥digo, imagens, logs, cache
- **Depend√™ncias**: Aguarda Ollama estar saud√°vel

#### 3. Model Downloader (model-downloader)
- **Execu√ß√£o**: Uma vez na inicializa√ß√£o
- **Fun√ß√£o**: Baixar modelos essenciais automaticamente
- **Modelos**: llama3.2:latest, phi3:mini
- **Restart**: "no" (executa uma vez)

#### 4. Nginx (nginx) - Opcional
- **Perfil**: production
- **Fun√ß√£o**: Proxy reverso e SSL
- **Portas**: 80, 443
- **Configura√ß√£o**: nginx.conf

### Volumes Persistentes

```yaml
volumes:
  ollama_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./docker-data/ollama
  huggingface_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./docker-data/huggingface
  transformers_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./docker-data/transformers
```

## üìÅ Estrutura do Projeto

```
LLM pessoal/
‚îú‚îÄ‚îÄ üê≥ Docker Files
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                 # Container da aplica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml         # Orquestra√ß√£o de servi√ßos
‚îÇ   ‚îî‚îÄ‚îÄ docker-entrypoint.sh       # Script de inicializa√ß√£o
‚îÇ
‚îú‚îÄ‚îÄ üöÄ Scripts de Execu√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ start-docker.bat           # Windows (Batch)
‚îÇ   ‚îî‚îÄ‚îÄ start-docker.ps1           # Windows (PowerShell)
‚îÇ
‚îú‚îÄ‚îÄ ‚öôÔ∏è Configura√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ env.example                # Exemplo de configura√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt           # Depend√™ncias Python
‚îÇ   ‚îî‚îÄ‚îÄ config.py                  # Configura√ß√µes da aplica√ß√£o
‚îÇ
‚îú‚îÄ‚îÄ üì± Aplica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ app.py                     # Servidor principal
‚îÇ   ‚îú‚îÄ‚îÄ static/                    # CSS, JS, imagens
‚îÇ   ‚îú‚îÄ‚îÄ templates/                 # Templates HTML
‚îÇ   ‚îî‚îÄ‚îÄ generated_images/          # Imagens geradas
‚îÇ
‚îî‚îÄ‚îÄ üíæ Dados Persistentes
    ‚îú‚îÄ‚îÄ docker-data/
    ‚îÇ   ‚îú‚îÄ‚îÄ ollama/                # Modelos Ollama
    ‚îÇ   ‚îú‚îÄ‚îÄ huggingface/           # Cache Hugging Face
    ‚îÇ   ‚îî‚îÄ‚îÄ transformers/          # Cache Transformers
    ‚îî‚îÄ‚îÄ logs/                      # Logs da aplica√ß√£o
```

## ‚ö° Comandos √öteis

### Windows PowerShell
```powershell
# Iniciar aplica√ß√£o
.\start-docker.ps1

# Ver status
.\start-docker.ps1 -Status

# Ver logs
.\start-docker.ps1 -Logs

# Parar aplica√ß√£o
.\start-docker.ps1 -Stop

# Reconstruir containers
.\start-docker.ps1 -Rebuild

# Pular download de modelos
.\start-docker.ps1 -NoModels
```

### Comandos Docker Diretos
```bash
# Iniciar todos os servi√ßos
docker-compose up -d

# Ver logs em tempo real
docker-compose logs -f

# Ver logs de um servi√ßo espec√≠fico
docker-compose logs -f llm-app

# Parar todos os servi√ßos
docker-compose down

# Reconstruir e iniciar
docker-compose up -d --build

# Ver status dos containers
docker-compose ps

# Executar comando no container
docker-compose exec llm-app bash

# Ver uso de recursos
docker stats

# Limpar volumes (cuidado!)
docker-compose down -v
```

### Gest√£o de Modelos Ollama
```bash
# Listar modelos instalados
curl http://localhost:11434/api/tags

# Baixar modelo espec√≠fico
curl -X POST http://localhost:11434/api/pull \
  -H "Content-Type: application/json" \
  -d '{"name": "llama3.2:latest"}'

# Executar comando no container Ollama
docker-compose exec ollama ollama list
docker-compose exec ollama ollama pull codellama:latest

# Ver informa√ß√µes do modelo
docker-compose exec ollama ollama show llama3.2:latest
```

## üîë Configura√ß√µes Avan√ßadas

### Arquivo .env (Opcional)

Copie `env.example` para `.env` e ajuste:

```bash
# Configura√ß√µes b√°sicas
HOST=0.0.0.0
PORT=8001
OLLAMA_HOST=http://ollama:11434

# Stable Diffusion
STABLE_DIFFUSION_MODEL=runwayml/stable-diffusion-v1-5
DEVICE=auto

# Performance
WORKERS=1
KEEP_ALIVE=5

# Debug (desenvolvimento)
DEBUG=true
LOG_LEVEL=DEBUG

# Windows/WSL
WSL_OPTIMIZATION=true
```

### Configura√ß√µes Docker Compose

#### Personalizar Portas
```yaml
services:
  llm-app:
    ports:
      - "8080:8001"  # Mudar porta externa
```

#### Adicionar Vari√°veis de Ambiente
```yaml
services:
  llm-app:
    environment:
      - CUSTOM_VAR=value
      - ANOTHER_VAR=value
```

#### Configurar Recursos
```yaml
services:
  llm-app:
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
```

## üîß Troubleshooting

### Problemas Comuns

#### Container n√£o inicia
```bash
# Ver logs detalhados
docker-compose logs llm-app

# Verificar se portas est√£o livres
netstat -an | grep 8001

# Reconstruir imagem
docker-compose build --no-cache
```

#### Ollama n√£o conecta
```bash
# Verificar se Ollama est√° rodando
docker-compose ps ollama

# Ver logs do Ollama
docker-compose logs ollama

# Reiniciar apenas Ollama
docker-compose restart ollama
```

#### Erro de mem√≥ria
```bash
# Verificar uso de mem√≥ria
docker stats

# Limpar cache n√£o usado
docker system prune -a

# Aumentar mem√≥ria do Docker (Windows/macOS)
# Docker Desktop > Settings > Resources > Memory
```

#### Modelos n√£o baixam
```bash
# Verificar logs do downloader
docker-compose logs model-downloader

# Baixar manualmente
docker-compose exec ollama ollama pull llama3.2:latest

# Verificar espa√ßo em disco
df -h
```

### Logs e Debug

#### Verificar Logs
```bash
# Logs de todos os servi√ßos
docker-compose logs

# Logs em tempo real
docker-compose logs -f

# Logs de servi√ßo espec√≠fico
docker-compose logs -f llm-app

# √öltimas 100 linhas
docker-compose logs --tail=100
```

#### Modo Debug
```yaml
services:
  llm-app:
    environment:
      - DEBUG=true
      - LOG_LEVEL=DEBUG
```

### Performance

#### Otimiza√ß√µes para GPU
```yaml
services:
  llm-app:
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
```

#### Otimiza√ß√µes para CPU
```yaml
services:
  llm-app:
    environment:
      - DEVICE=cpu
      - ENABLE_MEMORY_EFFICIENT_ATTENTION=false
```

## üìä Monitoramento

### Health Checks
- **Ollama**: Verifica se responde na API
- **LLM App**: Verifica se a aplica√ß√£o est√° online
- **Intervalo**: 30 segundos
- **Timeout**: 10 segundos

### M√©tricas Dispon√≠veis
```bash
# Status da aplica√ß√£o
curl http://localhost:8001/api/status

# Health check
curl http://localhost:8001/api/health

# Informa√ß√µes do sistema
curl http://localhost:8001/api/system-info
```

## üîÑ Manuten√ß√£o

### Atualiza√ß√µes
```bash
# Atualizar c√≥digo
git pull origin main

# Reconstruir containers
docker-compose build --no-cache

# Reiniciar servi√ßos
docker-compose up -d
```

### Backup
```bash
# Backup dos dados
tar -czf backup-$(date +%Y%m%d).tar.gz docker-data/ logs/

# Restaurar backup
tar -xzf backup-20231201.tar.gz
```

### Limpeza
```bash
# Limpar containers parados
docker container prune

# Limpar imagens n√£o usadas
docker image prune

# Limpar volumes n√£o usados
docker volume prune

# Limpeza completa
docker system prune -a
```

## üöÄ Produ√ß√£o

### Configura√ß√£o para Produ√ß√£o
```yaml
# docker-compose.prod.yml
version: '3.8'
services:
  nginx:
    profiles: ["production"]
    # ... configura√ß√£o nginx
```

### SSL/HTTPS
```yaml
services:
  nginx:
    volumes:
      - ./ssl:/etc/nginx/ssl:ro
    ports:
      - "443:443"
```

### Load Balancing
```yaml
services:
  llm-app:
    deploy:
      replicas: 3
```

## üìö Recursos Adicionais

### Documenta√ß√£o
- [Docker Compose](https://docs.docker.com/compose/)
- [Ollama](https://ollama.com/docs)
- [FastAPI](https://fastapi.tiangolo.com/)

### Comunidade
- [GitHub Issues](https://github.com/seu-repo/issues)
- [Discord](https://discord.gg/seu-servidor)

---

**üê≥ Docker torna a LLM Pessoal f√°cil de usar em qualquer sistema!** 