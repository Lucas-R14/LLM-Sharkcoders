# ï¿½ï¿½ LLM Pessoal - Guia Completo para Windows

## ğŸ‰ INSTALAÃ‡ÃƒO COMPLETA! âœ…

ParabÃ©ns! A sua **LLM Pessoal** estÃ¡ instalada e configurada no Windows. Esta Ã© uma aplicaÃ§Ã£o completa de IA local que combina chat inteligente e geraÃ§Ã£o de imagens.

## ğŸš€ Como Iniciar

### MÃ©todo 1: Script Batch (Recomendado)
```cmd
start_windows.bat
```

### MÃ©todo 2: Script PowerShell
```powershell
.\start_windows.ps1
```

### MÃ©todo 3: Manual
```cmd
venv\Scripts\python.exe app.py
```

## ğŸŒ Aceder Ã  AplicaÃ§Ã£o

Abra o seu navegador e vÃ¡ para: **http://localhost:8001**

## ğŸ“‹ Funcionalidades DisponÃ­veis

### ğŸ’¬ Chat Inteligente
- âœ… **Interface moderna** de chat com streaming
- âœ… **MÃºltiplos modelos** (LLaMA 3.2, Phi-3, Mistral, Code Llama)
- âœ… **HistÃ³rico de conversaÃ§Ã£o** persistente
- âœ… **Troca de modelos** em tempo real
- â³ **Modelos Ollama** (necessÃ¡rio download)

### ğŸ¨ GeraÃ§Ã£o de Imagens
- âœ… **Stable Diffusion** integrado
- âœ… **Controlos avanÃ§ados** (dimensÃµes, passos, guidance)
- âœ… **Prompt negativo** para melhor qualidade
- âœ… **Download de imagens** em alta resoluÃ§Ã£o
- ğŸ’» **Funciona com CPU** (mais lento mas funciona)

### âš™ï¸ ConfiguraÃ§Ãµes e Monitoramento
- âœ… **Status em tempo real** dos serviÃ§os
- âœ… **InformaÃ§Ãµes do sistema** (CPU, RAM, GPU)
- âœ… **GestÃ£o de histÃ³rico** e limpeza
- âœ… **Logs detalhados** para troubleshooting

## ğŸ¦™ Configurar Modelos Ollama

Para o chat funcionar, precisa de baixar modelos:

### Modelos Recomendados (por ordem de prioridade)

```cmd
# 1. Modelo pequeno e rÃ¡pido (recomendado para comeÃ§ar)
ollama pull phi3:mini

# 2. Modelo mais poderoso (precisa mais RAM)
ollama pull llama3.2:latest

# 3. Modelo especializado em cÃ³digo
ollama pull codellama:latest

# 4. Modelo rÃ¡pido e eficiente
ollama pull mistral:latest
```

### Verificar Modelos Instalados
```cmd
ollama list
```

### InformaÃ§Ãµes dos Modelos
```cmd
# Ver detalhes de um modelo
ollama show llama3.2:latest

# Ver uso de memÃ³ria
ollama ps
```

## ğŸ’¡ Guia de UtilizaÃ§Ã£o Detalhado

### Para Chat Inteligente

#### 1. Primeira ConfiguraÃ§Ã£o
1. **Aguarde o download** de pelo menos um modelo Ollama
2. **Verifique o status** na aba ConfiguraÃ§Ãµes
3. **Selecione um modelo** no dropdown

#### 2. Fluxo de Conversa
1. **Escolha o modelo** no seletor (ex: phi3:mini)
2. **Digite sua mensagem** no campo de texto
3. **Pressione Enter** ou clique em "Enviar"
4. **Aguarde a resposta** (aparece em streaming)
5. **Continue a conversa** naturalmente

#### 3. Dicas para Melhores Resultados
- **Seja especÃ­fico**: "Explique como funciona a fotossÃ­ntese" vs "fotossÃ­ntese"
- **Use contexto**: O modelo lembra da conversa atual
- **Limpe histÃ³rico**: Use o botÃ£o de lixeira quando necessÃ¡rio
- **Troque modelos**: Experimente diferentes modelos para diferentes tarefas

### Para GeraÃ§Ã£o de Imagens

#### 1. Interface de Imagens
- **Prompt Principal**: Descreva a imagem desejada
- **Prompt Negativo**: Especifique o que nÃ£o deve aparecer
- **Controles de Tamanho**: 512x512, 768x768, 1024x1024
- **ParÃ¢metros AvanÃ§ados**: Passos e escala de orientaÃ§Ã£o

#### 2. Fluxo de GeraÃ§Ã£o
1. **VÃ¡ para a aba "GeraÃ§Ã£o de Imagens"**
2. **Descreva a imagem** no campo principal
3. **Adicione prompt negativo** (opcional)
4. **Ajuste parÃ¢metros** se necessÃ¡rio:
   - **Passos**: 10-50 (mais = melhor qualidade, mais lento)
   - **Escala**: 1-20 (mais = mais fiel ao prompt)
5. **Clique em "Gerar Imagem"**
6. **Aguarde a geraÃ§Ã£o** (30-120 segundos)
7. **FaÃ§a download** da imagem gerada

#### 3. Dicas para Imagens
- **Prompts detalhados**: "Um gato siamÃªs sentado em um jardim japonÃªs ao pÃ´r do sol, estilo realista"
- **Prompts negativos**: "borrÃ£o, baixa qualidade, distorÃ§Ã£o, arte digital"
- **Tamanhos**: 512x512 Ã© mais rÃ¡pido, 1024x1024 Ã© mais detalhado
- **Passos**: 20-30 Ã© um bom equilÃ­brio

## âš¡ Performance e OtimizaÃ§Ã£o

### Com CPU apenas
- **Chat**: Funciona bem com modelos pequenos (phi3:mini)
- **Imagens**: 2-5 minutos por imagem
- **RAM**: 8GB mÃ­nimo, 16GB recomendado

### Com GPU NVIDIA
- **Chat**: Muito mais rÃ¡pido
- **Imagens**: 30-60 segundos por imagem
- **VRAM**: 4GB mÃ­nimo, 8GB+ recomendado

### OtimizaÃ§Ãµes para Windows
```cmd
# Verificar se CUDA estÃ¡ disponÃ­vel
python -c "import torch; print(torch.cuda.is_available())"

# Instalar PyTorch com CUDA (se necessÃ¡rio)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

## ğŸ”§ ResoluÃ§Ã£o de Problemas

### Ollama nÃ£o Responde
```cmd
# Verificar se estÃ¡ em execuÃ§Ã£o
tasklist | findstr ollama

# Reiniciar se necessÃ¡rio
taskkill /f /im ollama.exe
ollama serve

# Verificar porta
netstat -an | findstr 11434
```

### AplicaÃ§Ã£o nÃ£o Inicia
```cmd
# Verificar dependÃªncias
venv\Scripts\python.exe -c "import fastapi, torch, diffusers, ollama"

# Reinstalar se necessÃ¡rio
venv\Scripts\python.exe -m pip install -r requirements.txt

# Verificar logs
type logs\app.log
```

### Stable Diffusion Lento
- Use dimensÃµes menores (512x512)
- Reduza passos de inferÃªncia para 10-15
- Considere uma GPU NVIDIA para melhor performance
- Feche outras aplicaÃ§Ãµes para liberar RAM

### Erro de MemÃ³ria
```cmd
# Verificar uso de memÃ³ria
wmic OS get TotalVisibleMemorySize,FreePhysicalMemory

# Limpar cache Python
venv\Scripts\python.exe -m pip cache purge
```

### Porta em Uso
```cmd
# Verificar portas
netstat -an | findstr 8001

# Mudar porta (editar app.py ou usar variÃ¡vel)
set PORT=8080
venv\Scripts\python.exe app.py
```

## ğŸ“‚ Estrutura dos Ficheiros

```
LLM pessoal/
â”œâ”€â”€ ğŸ¯ start_windows.bat      # Script de arranque (Batch)
â”œâ”€â”€ ğŸ¯ start_windows.ps1      # Script de arranque (PowerShell)
â”œâ”€â”€ ğŸ“± app.py                 # AplicaÃ§Ã£o principal
â”œâ”€â”€ âš™ï¸ config.py              # ConfiguraÃ§Ãµes
â”œâ”€â”€ ğŸ“‹ requirements.txt       # DependÃªncias Python
â”œâ”€â”€ ğŸŒ templates/             # Interface web
â”‚   â””â”€â”€ index.html           # Template principal
â”œâ”€â”€ ğŸ¨ static/                # CSS e JavaScript
â”‚   â”œâ”€â”€ css/style.css        # Estilos
â”‚   â””â”€â”€ js/app.js           # JavaScript
â”œâ”€â”€ ğŸ“¸ generated_images/      # Imagens geradas
â”œâ”€â”€ ğŸ“Š logs/                 # Logs da aplicaÃ§Ã£o
â””â”€â”€ ğŸ—‚ï¸ venv/                 # Ambiente virtual Python
```

## ğŸ†˜ Suporte e Troubleshooting

### VerificaÃ§Ãµes BÃ¡sicas
1. **Verifique se todos os serviÃ§os estÃ£o a funcionar**
2. **Consulte esta documentaÃ§Ã£o**
3. **Reinicie a aplicaÃ§Ã£o**
4. **Verifique se tem espaÃ§o em disco suficiente**

### Logs e Debug
```cmd
# Ver logs da aplicaÃ§Ã£o
type logs\app.log

# Ver logs em tempo real
powershell "Get-Content logs\app.log -Wait"

# Modo debug
set DEBUG=true
venv\Scripts\python.exe app.py
```

### InformaÃ§Ãµes do Sistema
```cmd
# Verificar Python
venv\Scripts\python.exe --version

# Verificar PyTorch
venv\Scripts\python.exe -c "import torch; print(torch.__version__)"

# Verificar CUDA
venv\Scripts\python.exe -c "import torch; print(torch.cuda.is_available())"
```

## ğŸš€ PrÃ³ximos Passos

### 1. ConfiguraÃ§Ã£o Inicial
1. **Baixe modelos Ollama** para usar o chat
2. **Experimente gerar imagens** com Stable Diffusion
3. **Explore as configuraÃ§Ãµes** para optimizar a performance
4. **Personalize prompts** para melhores resultados

### 2. OtimizaÃ§Ãµes AvanÃ§adas
- **GPU NVIDIA**: Instale drivers CUDA para melhor performance
- **RAM**: Considere 16GB+ para modelos maiores
- **SSD**: Use SSD para melhor velocidade de cache
- **AntivÃ­rus**: Adicione exceÃ§Ãµes para Python e Ollama

### 3. PersonalizaÃ§Ã£o
- **Modelos customizados**: Experimente diferentes modelos Ollama
- **Prompts**: Crie prompts personalizados para suas necessidades
- **Interface**: Personalize CSS se necessÃ¡rio
- **ConfiguraÃ§Ãµes**: Ajuste parÃ¢metros no config.py

## ğŸ¯ InÃ­cio RÃ¡pido

1. **Execute**: `start_windows.bat`
2. **Aguarde** a aplicaÃ§Ã£o iniciar (1-2 minutos)
3. **Abra**: http://localhost:8001
4. **Baixe um modelo**: `ollama pull phi3:mini`
5. **Divirta-se!** ğŸ‰

**âœ¨ A sua LLM pessoal estÃ¡ pronta para usar! ğŸš€**

---

*Desenvolvido especificamente para Windows com otimizaÃ§Ãµes nativas.* 