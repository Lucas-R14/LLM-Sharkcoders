version: '3.8'

services:
  llm-app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: llm-pessoal:latest
    container_name: llm-app
    restart: unless-stopped
    
    # Gestão de recursos
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '1.0'
          memory: 2G
    
    # Variáveis de ambiente
    environment:
      - HOST=0.0.0.0
      - PORT=8001
      - DEBUG=false
      - OLLAMA_HOST=http://ollama:11434
      - WHISPER_CACHE_DIR=/app/cache/whisper
      - HF_CACHE_DIR=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/transformers
      - HF_HUB_DISABLE_SYMLINKS=1
      - HF_HUB_DISABLE_SYMLINKS_WARNING=1
      - PYTORCH_ENABLE_MPS_FALLBACK=1
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
    
    # Volumes persistentes
    volumes:
      - whisper_cache:/app/cache/whisper
      - huggingface_cache:/app/cache/huggingface
      - transformers_cache:/app/cache/transformers
      - generated_images:/app/generated_images
      - app_logs:/app/logs
    
    # Portas
    ports:
      - "8001:8001"
    
    # Dependências
    depends_on:
      ollama:
        condition: service_healthy
    
    # Redes
    networks:
      - llm-network
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Logs
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    
    # GPU support (descomente se tiver GPU NVIDIA)
    # runtime: nvidia
    # environment:
    #   - NVIDIA_VISIBLE_DEVICES=all
    
    # Gestão de recursos
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G
    
    # Volumes
    volumes:
      - ollama_data:/root/.ollama
    
    # Portas
    ports:
      - "11434:11434"
    
    # Redes
    networks:
      - llm-network
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/status"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    
    # Logs
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Serviço de monitorização (opcional)
  watchtower:
    image: containrrr/watchtower:latest
    container_name: watchtower
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_POLL_INTERVAL=3600
      - WATCHTOWER_INCLUDE_STOPPED=true
    networks:
      - llm-network

# Volumes nomeados para persistência
volumes:
  whisper_cache:
    driver: local
  huggingface_cache:
    driver: local
  transformers_cache:
    driver: local
  generated_images:
    driver: local
  app_logs:
    driver: local
  ollama_data:
    driver: local

# Rede personalizada
networks:
  llm-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16 